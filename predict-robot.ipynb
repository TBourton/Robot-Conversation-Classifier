{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport tqdm.notebook as tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom matplotlib import MatplotlibDeprecationWarning\n\nwarnings.filterwarnings(\"ignore\", category=MatplotlibDeprecationWarning)\n\nSEED = 0\nDF_PATH = '../input/classification-of-robots-from-their-conversation/Classification of Robots from their conversation sequence.csv'\nT_COLS = [f\"num{i+1}\" for i in range(10)]\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(DF_PATH)\ndf['kind'] = [(i // 5) for i in range(len(df))]\ndf.head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('source').describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grad_df = pd.DataFrame(np.gradient(df[T_COLS].to_numpy(), axis=1), columns=[f'grad_{x}' for x in T_COLS])\ngrad_df['source'] = df['source']\ngrad_df.groupby('source').describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There is a lot of data - pick some random samples\nrandom.seed(SEED)\nSUB_SAMPLE_FRAC = 0.001\n\nkinds = df.kind.unique()\n\nk = int(len(kinds) * SUB_SAMPLE_FRAC)\nkinds = random.choices(kinds, k=k)\n\ndf = df[df.kind.isin(kinds)]\ndf.head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train test split\nTEST_SIZE = 0.2\nrandom.seed(SEED)\n\nkinds = df.kind.unique()\nk = int(len(kinds) * TEST_SIZE)\ntest_kinds = random.choices(kinds, k=k)\ntrain_kinds = kinds[~np.isin(kinds, test_kinds)]\n\n# Split and shuffle (sample(1))\ndf_train = df[df.kind.isin(train_kinds)].sample(frac=1).copy()\ndf_test = df[df.kind.isin(test_kinds)].sample(frac=1).copy()\n\nassert len(df) == len(df_train) + len(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnd_kinds = random.choices(df_train.kind.unique(), k=5)\ntemp_df = pd.melt(df_train, id_vars=['source', 'kind'], value_vars=T_COLS, var_name='time')\nfig, ax = plt.subplots(1, 5, figsize=(25, 5))\nfor i, axx in enumerate(ax):\n    for kind, gp in temp_df[(temp_df.source == i) & (temp_df.kind.isin(rnd_kinds))].groupby('kind'):\n        gp.plot(x='time', y='value', ax=axx, label=kind, title=f'Robot {i}, by example.', sharex=True)\n\nplt.show()        \nplt.close()\ndel temp_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distinguisting between 0 vs (1 or 3) vs (2 or 4) seems easy, but distinguishing (1 from 3) or (2 from 4) not as much.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nzscaler = StandardScaler()\n\n# fit and transform the data - we are careful to only fit the scaler on the train data, so as not to cause leakage\ndf_train[T_COLS] = zscaler.fit_transform(df_train[T_COLS])\ndf_test[T_COLS] = zscaler.transform(df_test[T_COLS])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df = pd.melt(df_train, id_vars=['source', 'kind'], value_vars=T_COLS, var_name='time')\nfig, ax = plt.subplots(1, 5, figsize=(25, 5))\nfor i, axx in enumerate(ax):\n    for kind, gp in temp_df[(temp_df.source == i) & (temp_df.kind.isin(rnd_kinds))].groupby('kind'):\n        gp.plot(x='time', y='value', ax=axx, label=kind, title=f'Robot {i}, by example.', sharex=True)\n\nplt.show()\nplt.close()\ndel temp_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature extraction w/ tsfresh\n\nExtract features with tsfresh, train model.","metadata":{}},{"cell_type":"code","source":"# Prepare new df for tsfresh\nts_test = pd.melt(df_test, id_vars=['source', 'kind'], value_vars=T_COLS, var_name='time')\nts_train = pd.melt(df_train, id_vars=['source', 'kind'], value_vars=T_COLS, var_name='time')\n\nts_test.time = ts_test.time.apply(lambda x: int(x.strip('num')))\nts_train.time = ts_train.time.apply(lambda x: int(x.strip('num')))\n\n# We want to predict 'source' as dependent variable, make a new variable and drop it from dataframe.\ny_train, y_test = ts_train.source, ts_test.source\nts_train['id'] = ts_train.index\nts_train = ts_train.drop(columns=['source', 'kind'])\nts_test['id'] = ts_test.index\nts_test = ts_test.drop(columns=['source', 'kind'])\n\nprint(y_train.values[:12])\nts_train.head(12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tsfresh\nfrom tsfresh import extract_relevant_features\nfrom tsfresh.feature_extraction import extract_features\nfrom tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n\nextraction_settings = ComprehensiveFCParameters()\n# extraction_settings = MinimalFCParameters()\n\nx_train = extract_relevant_features(\n    ts_train,\n    y_train,\n    column_id='id',\n    column_sort='time',\n    default_fc_parameters=extraction_settings,\n)\n\nx_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = x_train.copy()\ncorr['y'] = y_train\ncorr = corr.corr()\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)\nplt.show()\ncorr['y'].drop('y').abs().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_parameters = tsfresh.feature_extraction.settings.from_columns(x_train)\nx_test = extract_features(\n    ts_test,\n    column_id='id',\n    column_sort='time',\n    kind_to_fc_parameters=train_parameters\n)\nx_test.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.metrics import classification_report as cr, confusion_matrix as cm\n\nfrom xgboost import XGBClassifier\n\nrf = RandomForestClassifier(random_state=0).fit(x_train, y_train)\ny_pred = rf.predict(x_test)\nprint(\"RF\")\nprint(cr(y_pred=y_pred, y_true=y_test))\nprint(cm(y_pred=y_pred, y_true=y_test))\n\nrf_bagged = BaggingClassifier(base_estimator=RandomForestClassifier(), random_state=0).fit(x_train, y_train)\ny_pred = rf_bagged.predict(x_test)\nprint(\"RF Bagged\")\nprint(cr(y_pred=y_pred, y_true=y_test))\nprint(cm(y_pred=y_pred, y_true=y_test))\n\nxgbc = XGBClassifier(use_label_encoder=False).fit(x_train, y_train)\ny_pred = xgbc.predict(x_test)\nprint(\"XGBC\")\nprint(cr(y_pred=y_pred, y_true=y_test))\nprint(cm(y_pred=y_pred, y_true=y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, the the model is good at predicting 0 vs (1 or 3) vs (2 or 4) but often gets confused between (1 vs 3) or (2 vs 4).\n\nIndeed, Kolmogorv-Smirnov test shows that 1 & 3 are very likely to be the same distribution (high value for all columns) 2 and 4 are different though\n\nhttps://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test","metadata":{}},{"cell_type":"code","source":"from scipy.stats import ks_2samp\n\ndf = pd.read_csv(DF_PATH)\n\nprint(\"Cols 1 vs 3\")\nfor col in T_COLS:\n    print(ks_2samp(df[df.source == 1][col].values, df[df.source == 3][col].values))\n    \nprint(\"\\nCols 2 vs 4\")\nfor col in T_COLS:\n    print(ks_2samp(df[df.source == 2][col].values, df[df.source == 4][col].values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = x_train[y_train != 3]\nx_test = x_test[y_test != 3]\ny_train = y_train[y_train != 3]\ny_test = y_test[y_test != 3]\n\ny_train[y_train == 4] = 3\ny_test[y_test == 4] = 3\n\n\nrf = RandomForestClassifier(random_state=0).fit(x_train, y_train)\ny_pred = rf.predict(x_test)\nprint(\"RF\")\nprint(cr(y_pred=y_pred, y_true=y_test))\nprint(cm(y_pred=y_pred, y_true=y_test))\n\nrf_bagged = BaggingClassifier(base_estimator=RandomForestClassifier(), random_state=0).fit(x_train, y_train)\ny_pred = rf_bagged.predict(x_test)\nprint(\"RF Bagged\")\nprint(cr(y_pred=y_pred, y_true=y_test))\nprint(cm(y_pred=y_pred, y_true=y_test))\n\nxgbc = XGBClassifier(use_label_encoder=False).fit(x_train, y_train)\ny_pred = xgbc.predict(x_test)\nprint(\"XGBC\")\nprint(cr(y_pred=y_pred, y_true=y_test))\nprint(cm(y_pred=y_pred, y_true=y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FCN\n\nhttps://keras.io/examples/timeseries/timeseries_classification_from_scratch/","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\n\ndef make_model(input_shape, num_classes, num_layers=1):\n    \"\"\"From https://arxiv.org/abs/1611.06455.\"\"\"\n    input_layer = x = keras.layers.Input(input_shape)\n\n    for _ in range(num_layers):\n        x = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(x)\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.ReLU()(x)\n    \n    x = keras.layers.Dropout(0.1)(x)\n    x = keras.layers.GlobalAveragePooling1D()(x)\n\n    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n\n    return keras.models.Model(inputs=input_layer, outputs=output_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nTEST_SIZE = 0.2\n\ndf = pd.read_csv(DF_PATH)\n\ndf = df[df.source != 3]  # Remove robot 3, since it's the same as 1, as we already proved\ndf.loc[df.source == 4, 'source'] = 3 #.values = 3  # Also rename claess 4 to class 3 so we can still use sparsecrossentropy\n\nx, y = df[T_COLS].to_numpy(), df['source'].to_numpy()\nx_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, random_state=0, test_size=TEST_SIZE, shuffle=True)\n\nzscaler = StandardScaler()\n\n# fit and transform the data - we are careful to only fit the scaler on the train data, so as not to cause leakage\nx_train = zscaler.fit_transform(x_train)\nx_test = zscaler.transform(x_test)\n\n# Reshape\nx_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\nx_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\nLR = 1e-3\nEPOCHS = 500\nBATCH_SIZE = 128\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_loss\", verbose=1),\n    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=20, min_lr=1e-7, verbose=1),\n    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n]\n\nmodel = make_model(input_shape=x_train.shape[1:], num_classes=len(np.unique(y_train)))\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(LR),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"sparse_categorical_accuracy\"],\n)\n\nprint(model.summary())\n\nhistory = model.fit(\n    x_train,\n    y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    validation_split=0.2,\n    verbose=1,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report as cr, confusion_matrix as cm\n\nmodel = keras.models.load_model(\"best_model.h5\")\n\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Test accuracy\", test_acc)\nprint(\"Test loss\", test_loss)\n\ny_pred = model.predict(x_test)\ny_pred = np.argmax(y_pred, axis=1)\nprint(cr(y_pred=y_pred, y_true=y_test))\nprint(cm(y_pred=y_pred, y_true=y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = [\"sparse_categorical_accuracy\", \"loss\"]\nfig, axs = plt.subplots(1, len(metrics), figsize=(16, 16/len(metrics)))\nfor metric, ax in zip(metrics, axs):\n    ax.plot(history.history[metric])\n    ax.plot(history.history[\"val_\" + metric])\n    ax.set_title(\"model \" + metric)\n    ax.set(ylabel=metric, xlabel='epoch')\n    ax.legend([\"train\", \"val\"], loc=\"best\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After deleting robot 3, this model solves this task quite easily.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}